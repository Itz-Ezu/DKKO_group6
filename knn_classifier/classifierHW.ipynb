{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2966e82c09c9183",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Breast Cancer Diagnosis\n",
    "\n",
    "## Assignment understanding\n",
    "\n",
    "This work based of the idea that it is possible to develop a computer system to help diagnose breast cancer. The system analyzes data derived from images of cell samples, which are taken from a breast lump. Instead of a doctor relying solely on a subjective judgment, this system uses machine learning to classify the cells as cancerous or not.\n",
    "\n",
    "The primary business objective in any clinical diagnostic setting is to maximize diagnostic accuracy while minimizing errors. Therefore, the goal is to develop a model that serves as a <del>somehow</del> reliable decision support tool for clinicians.\n",
    "\n",
    "For these purpose we will use the **kNN-classifier algorithm** combined with **hold-out validation** to train a binary classifier. Our process involves tuning the model's hyperparameters by exploring a **range of K-values** to find the configuration that provides the **best possible performance**. Result is measured by use of confusion matrix and key metrics like accuracy, precision, and recall.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data understanding\n",
    "\n",
    "The dataset contains diagnostic test results from 569 patients. It is comprised of 357 samples classified as benign and 212 as malignant. This class distribution is slightly imbalanced, but it is not expected to be a critical issue for this analysis. According to the source, the dataset is clean and contains no missing values.\n",
    "\n",
    "### Provided specifications\n",
    "\n",
    "#### Data consists of 32 attributes:\n",
    "- 1 (ID)\n",
    "- 1 target variable (diagnosis)\n",
    "- 30 numeric variables\n",
    "\n",
    "#### Target variables:\n",
    "- B (Benign)\n",
    "- M (Malignant)\n",
    "\n",
    "#### Measured features (numeric):\n",
    "\n",
    "| Measurement      | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| Radius           | The mean distance from the center to points on the perimeter.               |\n",
    "| Texture          | The standard deviation of gray-scale values.                                |\n",
    "| Perimeter        | The length of the nucleus boundary.                                         |\n",
    "| Area             | The area of the nucleus.                                                    |\n",
    "| Smoothness       | A measure of the local variation in radius lengths.                         |\n",
    "| Compactness      | Calculated as (perimeterÂ² / area) - 1.0.                                    |\n",
    "| Concavity        | The severity of concave portions of the contour.                            |\n",
    "| Concave points   | The number of concave portions of the contour.                              |\n",
    "| Symmetry         | A measure of the nucleus's symmetry.                                        |\n",
    "| Fractal dimension| An approximation of the \"coastline\" complexity.                             |\n",
    "\n",
    "\n",
    "#### For each feature, three values are recorded:\n",
    "\n",
    "\n",
    "| Value Type | Description                                                                 |\n",
    "|------------|-----------------------------------------------------------------------------|\n",
    "| Mean       | The average of all nuclei in the image.                                     |\n",
    "| SE         | The standard error of the mean for the nuclei.                              |\n",
    "| Worst      | The mean of the three largest (\"worst\") values for that feature in the image.|\n"
   ],
   "id": "8f797f6afbc6ac4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "\n",
    "### Data import\n",
    "\n",
    "To ensure a consistent and reliable workflow, we are loading the data from a local folder. This approach makes our project self-contained and independent of network connections, which we found to be a more robust method in previous work."
   ],
   "id": "ec84836fd28a5f7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:13:31.434532Z",
     "start_time": "2025-09-05T05:13:31.392231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 10 available features\n",
    "features = [\n",
    "    \"radius\",\n",
    "    \"texture\",\n",
    "    \"perimeter\",\n",
    "    \"area\",\n",
    "    \"smoothness\",\n",
    "    \"compactness\",\n",
    "    \"concavity\",\n",
    "    \"concave_points\",\n",
    "    \"symmetry\",\n",
    "    \"fractal_dimension\"\n",
    "]\n",
    "\n",
    "# 3 versions of each feature available\n",
    "suffixes = [\"mean\", \"se\", \"worst\"]\n",
    "\n",
    "# Combining into unique column names\n",
    "columns = [\"ID\", \"Diagnosis\"]\n",
    "for suffix in suffixes:\n",
    "    for feature in features:\n",
    "        columns.append(f\"{feature}_{suffix}\")\n",
    "\n",
    "# Use the generated list to load the data into a pandas DataFrame\n",
    "df = pd.read_csv(\"./breastCancerData/wdbc.data\", header=None, names=columns)\n",
    "\n",
    "print(\"\\n--- Example Data ---\\n\")\n",
    "df.head()\n"
   ],
   "id": "304f6b13f5d8a2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Data ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         ID Diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data cleaning and validation\n",
    "\n",
    "Cleaning data by changing Diagnosis column from a text object to a categorical data type *(others have to be \"float\" values by default)* and dropping the ID column as it is not a predictive feature."
   ],
   "id": "46c67624063aa05f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:13:31.482620Z",
     "start_time": "2025-09-05T05:13:31.476514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"Diagnosis\"] = df[\"Diagnosis\"].astype(\"category\")\n",
    "clean_df = df.drop(columns=['ID'])"
   ],
   "id": "dfad9fbe340e0ca2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then perform a quick check to validate that the data is clean and ready for use. We inspect the data types and check for any missing values.",
   "id": "9518ab8dcafb59dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:13:31.561897Z",
     "start_time": "2025-09-05T05:13:31.530102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = clean_df.describe(include=\"all\").T  # Transpose for better readability\n",
    "summary[\"dtype\"] = df.dtypes # adding data types\n",
    "summary[\"missing\"] = df.isnull().sum() # adding amount of NaN values\n",
    "display(summary.T) # display the \"transposed summary\""
   ],
   "id": "c8d64a370f3a98f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Diagnosis radius_mean texture_mean perimeter_mean   area_mean  \\\n",
       "count         569       569.0        569.0          569.0       569.0   \n",
       "unique          2         NaN          NaN            NaN         NaN   \n",
       "top             B         NaN          NaN            NaN         NaN   \n",
       "freq          357         NaN          NaN            NaN         NaN   \n",
       "mean          NaN   14.127292    19.289649      91.969033  654.889104   \n",
       "std           NaN    3.524049     4.301036      24.298981  351.914129   \n",
       "min           NaN       6.981         9.71          43.79       143.5   \n",
       "25%           NaN        11.7        16.17          75.17       420.3   \n",
       "50%           NaN       13.37        18.84          86.24       551.1   \n",
       "75%           NaN       15.78         21.8          104.1       782.7   \n",
       "max           NaN       28.11        39.28          188.5      2501.0   \n",
       "dtype    category     float64      float64        float64     float64   \n",
       "missing         0           0            0              0           0   \n",
       "\n",
       "        smoothness_mean compactness_mean concavity_mean concave_points_mean  \\\n",
       "count             569.0            569.0          569.0               569.0   \n",
       "unique              NaN              NaN            NaN                 NaN   \n",
       "top                 NaN              NaN            NaN                 NaN   \n",
       "freq                NaN              NaN            NaN                 NaN   \n",
       "mean            0.09636         0.104341       0.088799            0.048919   \n",
       "std            0.014064         0.052813        0.07972            0.038803   \n",
       "min             0.05263          0.01938            0.0                 0.0   \n",
       "25%             0.08637          0.06492        0.02956             0.02031   \n",
       "50%             0.09587          0.09263        0.06154              0.0335   \n",
       "75%              0.1053           0.1304         0.1307               0.074   \n",
       "max              0.1634           0.3454         0.4268              0.2012   \n",
       "dtype           float64          float64        float64             float64   \n",
       "missing               0                0              0                   0   \n",
       "\n",
       "        symmetry_mean  ... radius_worst texture_worst perimeter_worst  \\\n",
       "count           569.0  ...        569.0         569.0           569.0   \n",
       "unique            NaN  ...          NaN           NaN             NaN   \n",
       "top               NaN  ...          NaN           NaN             NaN   \n",
       "freq              NaN  ...          NaN           NaN             NaN   \n",
       "mean         0.181162  ...     16.26919     25.677223      107.261213   \n",
       "std          0.027414  ...     4.833242      6.146258       33.602542   \n",
       "min             0.106  ...         7.93         12.02           50.41   \n",
       "25%            0.1619  ...        13.01         21.08           84.11   \n",
       "50%            0.1792  ...        14.97         25.41           97.66   \n",
       "75%            0.1957  ...        18.79         29.72           125.4   \n",
       "max             0.304  ...        36.04         49.54           251.2   \n",
       "dtype         float64  ...      float64       float64         float64   \n",
       "missing             0  ...            0             0               0   \n",
       "\n",
       "         area_worst smoothness_worst compactness_worst concavity_worst  \\\n",
       "count         569.0            569.0             569.0           569.0   \n",
       "unique          NaN              NaN               NaN             NaN   \n",
       "top             NaN              NaN               NaN             NaN   \n",
       "freq            NaN              NaN               NaN             NaN   \n",
       "mean     880.583128         0.132369          0.254265        0.272188   \n",
       "std      569.356993         0.022832          0.157336        0.208624   \n",
       "min           185.2          0.07117           0.02729             0.0   \n",
       "25%           515.3           0.1166            0.1472          0.1145   \n",
       "50%           686.5           0.1313            0.2119          0.2267   \n",
       "75%          1084.0            0.146            0.3391          0.3829   \n",
       "max          4254.0           0.2226             1.058           1.252   \n",
       "dtype       float64          float64           float64         float64   \n",
       "missing           0                0                 0               0   \n",
       "\n",
       "        concave_points_worst symmetry_worst fractal_dimension_worst  \n",
       "count                  569.0          569.0                   569.0  \n",
       "unique                   NaN            NaN                     NaN  \n",
       "top                      NaN            NaN                     NaN  \n",
       "freq                     NaN            NaN                     NaN  \n",
       "mean                0.114606       0.290076                0.083946  \n",
       "std                 0.065732       0.061867                0.018061  \n",
       "min                      0.0         0.1565                 0.05504  \n",
       "25%                  0.06493         0.2504                 0.07146  \n",
       "50%                  0.09993         0.2822                 0.08004  \n",
       "75%                   0.1614         0.3179                 0.09208  \n",
       "max                    0.291         0.6638                  0.2075  \n",
       "dtype                float64        float64                 float64  \n",
       "missing                    0              0                       0  \n",
       "\n",
       "[13 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>...</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.09636</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.26919</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.07972</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.981</td>\n",
       "      <td>9.71</td>\n",
       "      <td>43.79</td>\n",
       "      <td>143.5</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106</td>\n",
       "      <td>...</td>\n",
       "      <td>7.93</td>\n",
       "      <td>12.02</td>\n",
       "      <td>50.41</td>\n",
       "      <td>185.2</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.05504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.7</td>\n",
       "      <td>16.17</td>\n",
       "      <td>75.17</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.08637</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.02031</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01</td>\n",
       "      <td>21.08</td>\n",
       "      <td>84.11</td>\n",
       "      <td>515.3</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>0.07146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.37</td>\n",
       "      <td>18.84</td>\n",
       "      <td>86.24</td>\n",
       "      <td>551.1</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.09263</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>...</td>\n",
       "      <td>14.97</td>\n",
       "      <td>25.41</td>\n",
       "      <td>97.66</td>\n",
       "      <td>686.5</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.09993</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.78</td>\n",
       "      <td>21.8</td>\n",
       "      <td>104.1</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>...</td>\n",
       "      <td>18.79</td>\n",
       "      <td>29.72</td>\n",
       "      <td>125.4</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.3391</td>\n",
       "      <td>0.3829</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.09208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.11</td>\n",
       "      <td>39.28</td>\n",
       "      <td>188.5</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>0.3454</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>0.2012</td>\n",
       "      <td>0.304</td>\n",
       "      <td>...</td>\n",
       "      <td>36.04</td>\n",
       "      <td>49.54</td>\n",
       "      <td>251.2</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.252</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.2075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtype</th>\n",
       "      <td>category</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>...</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows Ã 31 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The summary confirms that there are no missing values and the data types are correct.\n",
    "\n",
    "### Splitting the Data for Training\n",
    "\n",
    "Finally, we divide the dataframe into features `X` and the target variable `y`. The target variable y is mapped from categorical ('B'/'M') to numeric (0/1) for compatibility with the machine learning model.\n",
    "\n",
    "Following the principles from our course materials, we split the data into a training set and a testing set using a 70/30 ratio. We use `stratify=y` to ensure that both the training and testing sets have the same proportion of benign and malignant samples as the original dataset. This is important for handling the slight class imbalance."
   ],
   "id": "ecd53d686a6c2778"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:13:31.648732Z",
     "start_time": "2025-09-05T05:13:31.637637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the features (inputs) from the target\n",
    "X = clean_df.drop(columns=['Diagnosis'])\n",
    "y = clean_df['Diagnosis'].map({'B': 0, 'M': 1})  # Map target to 0 and 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify = y)"
   ],
   "id": "7e23fbf77726833f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data standardisation\n",
    "\n",
    "Finally, we need to standardise the features. Algorithms like k-NN are sensitive to the scale of the data, and standardisation ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "To prevent data leakage and simulate a real-world scenario, we must fit the scaler only on the training data *(we didn't get it from the first try, but here we are)*. This step calculates the mean and standard deviation from the training set. We then use this same fitted scaler to transform both the training data and the test data."
   ],
   "id": "79555ab6d27290da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:13:31.723816Z",
     "start_time": "2025-09-05T05:13:31.718176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() # Initializing standard scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Scaling train inputs\n",
    "X_test_scaled = scaler.transform(X_test) # Using the SAME fitted scaler to scale test inputs"
   ],
   "id": "3eede91f36ecc0e3",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modeling\n",
    "\n",
    "In this section, we begin to tune our model by exploring its main *hyper*parameter, `k`. The value of `k` in a k-NN model determines how many neighbors are used to classify a new data point. A small `k` can make the model sensitive to noise (high variance), while a large `k` can over-generalize and miss local patterns (high bias).\n",
    "\n",
    "With this in mind, we decided to test only odd numbers for `k`, as an even number could result in a tie when classifying between two classes. Rather than performing an exhaustive automated search, we tested a few manual chosen key values to observe the trend in performance.\n",
    "\n",
    "**Important Note:** For this initial hyperparameter check, we are training and **predicting on the training set**. This helps us see how well the model learns the training data but **is not a good measure of how it will perform** on new, unseen data."
   ],
   "id": "71218417182abacd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:24:04.301211Z",
     "start_time": "2025-09-05T05:24:04.262258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in [3, 5, 7, 11]: # trying different values\n",
    "    kNN = KNeighborsClassifier(n_neighbors = k)\n",
    "    kNN.fit(X_train_scaled, y_train) # building the classifier\n",
    "\n",
    "    y_pred = kNN.predict(X_train_scaled)  # predict on training set\n",
    "    \n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    prec = precision_score(y_train, y_pred)\n",
    "    rec = recall_score(y_train, y_pred)\n",
    "\n",
    "    results.append({\"k\": k, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Results of Tuning on the Training Set ---\\n\")\n",
    "results_df"
   ],
   "id": "8a5edd2fd315968d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results of Tuning on the Training Set ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    k  Accuracy  Precision    Recall\n",
       "0   3  0.979899   0.992958  0.952703\n",
       "1   5  0.977387   0.992908  0.945946\n",
       "2   7  0.972362   0.992806  0.932432\n",
       "3  11  0.969849   0.992754  0.925676"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.979899</td>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.952703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.977387</td>\n",
       "      <td>0.992908</td>\n",
       "      <td>0.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.972362</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.932432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.969849</td>\n",
       "      <td>0.992754</td>\n",
       "      <td>0.925676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "75bdef89-ce77-44a2-93fa-8b5d23b7c3e1",
   "metadata": {},
   "source": [
    "As the table shows, there is a clear, small, decline in all performance metrics as `k` increases beyond 3.\n",
    "\n",
    "Based on these results, k=3 appears to be the best choice. In the context of cancer diagnosis, recall is arguably the most critical metric. Failing to identify a malignant case (a False Negative) has much more severe consequences than a \"false alarm\" (a False Positive). Since k=3 provides the highest recall and accuracy, it is our selected value.\n",
    "\n",
    "Now, we will use our chosen hyperparameter (k=3) to train a final model and evaluate its performance on the held-out test set. This gives us a true measure of how our model is expected to perform on new data."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:26:54.400590Z",
     "start_time": "2025-09-05T05:26:54.389087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# choosing the best value for k\n",
    "k_best = 3\n",
    "kNN_best = KNeighborsClassifier(n_neighbors = k_best)\n",
    "kNN_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_best = kNN_best.predict(X_test_scaled) # predict with testing set\n",
    "\n",
    "print(f\"\\n--- Final Results for k = {k_best} ---\\n\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_best))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_best))"
   ],
   "id": "7f88d26ea27e6198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results for k = 3 ---\n",
      "\n",
      "Accuracy: 0.9707602339181286\n",
      "Precision: 0.9836065573770492\n",
      "Recall: 0.9375\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Despite using a simplified method for hyperparameter tuning, the model still achieved a high accuracy of over 97% on the test set, although recall and precision saw a slight drop compared to the training set scores, which is expected.",
   "id": "b74077d1f4952a22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "For the final evaluation, we analyze the performance of our best model (k=3) on the held-out test set. While the overall accuracy score was high, a confusion matrix gives us a visualisation of errors the model is making."
   ],
   "id": "3e629b4988149413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['Benign', 'Malignant'])\n",
    "cmd.plot()"
   ],
   "id": "51bbc0754f77e29c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The matrix shows the following results on the 171 samples in the test set:\n",
    "- True Negatives (TN): 106 - Correctly identified benign cases.\n",
    "- False Positives (FP): 1 - A benign case incorrectly labeled as malignant (a \"false alarm\").\n",
    "- False Negatives (FN): 4 - Malignant cases incorrectly labeled as benign (a \"miss\").\n",
    "- True Positives (TP): 59 - Correctly identified malignant cases.\n",
    "\n",
    "Our analysis leads to the following conclusions:\n",
    "1) The model is very good at identifying benign tumors, making only one error in that class.\n",
    "2) The model's most critical weakness is the 4 False Negatives. In a real-world medical scenario, failing to detect a malignant tumor is the most dangerous type of error.\n",
    "3) The class imbalance in the original dataset (with fewer malignant samples) may have contributed to the model's difficulty in identifying every malignant case.\n",
    "\n",
    "While a recall of approximately 94% (59 out of 63 malignant cases identified) is statistically high, the four missed cases are a significant concern. For these reasons, we conclude that the model, in its current state, is not reliable enough for clinical use.\n",
    "\n",
    "## Deployment\n",
    "\n",
    "This model was **developed for educational purposes** to demonstrate the process of building and evaluating a classifier.\n",
    "\n",
    "Due to the **critical issue of False Negatives identified** during the evaluation, the **model is not safe or suitable for use** in any real-world clinical or business circumstance. Further work, potentially including gathering more data or using more advanced modeling techniques to improve recall, would be necessary before considering deployment."
   ],
   "id": "abfd86fae3910ebf"
  },
  {
   "cell_type": "markdown",
   "id": "54766e56a25030e0",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "#### Ai Usage\n",
    "Throughout this project, an AI assistant was utilized for the following tasks:\n",
    "\n",
    "- **Analysis of Medical Literature:** Assisting in the summary and explanation of the provided research papers that form the basis of the dataset.\n",
    "- **Conceptual Clarification:** Explaining key machine learning concepts and the role of each step in the CRISP-DM framework.\n",
    "- **Code and Methodology Review:** Identifying and correcting methodological errors in the project workflow, most notably the incorrect application of the data scaler on the test set.\n",
    "- **Text Refinement:** Editing and proofreading the project narrative for grammatical correctness, clarity, and an appropriate technical tone.\n",
    "\n",
    "#### Team contribution\n",
    "All team members contributed effectively to the project's completion. The team demonstrated great flexibility and maintained clear communication, especially when working under a tight timetable for some participants. This collaborative approach was essential to successfully completing the project.\n",
    "\n",
    "#### Additional sources used\n",
    "Hyperparameter Tuning for k-NN:\n",
    "- Agrawal, S. (2023). \"Hyperparameter Tuning of KNN Classifier.\" Medium. [Link](https://medium.com/%40agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7)\n",
    "- GeeksforGeeks. \"How to find the optimal value of K in K-NN?\" [Link](https://www.geeksforgeeks.org/machine-learning/how-to-find-the-optimal-value-of-k-in-knn/?utm_source=chatgpt.com)\n",
    "\n",
    "Confusion Matrix Visualization:\n",
    "- Scikit-learn documentation. \"Confusion Matrix.\" [Link](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html?utm_source=chatgpt.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
